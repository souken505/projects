{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The task for the NPPE was fine-tunning a llama 3.1 8b instruct model, to perform sentimental analysis for senetences in different Indian languages. \nThe following are the steps undertaken to make the model as efficent as possible.\n\nFist we download all the necessary packages","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install accelerate\n!pip install peft\n!pip install --upgrade transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:39:53.423419Z","iopub.execute_input":"2025-02-18T10:39:53.423799Z","iopub.status.idle":"2025-02-18T10:40:06.985575Z","shell.execute_reply.started":"2025-02-18T10:39:53.423775Z","shell.execute_reply":"2025-02-18T10:40:06.984269Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Now importing the packages","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    BitsAndBytesConfig,\n    TextClassificationPipeline\n)\nfrom peft import LoraConfig, get_peft_model\nimport torch\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:06.987251Z","iopub.execute_input":"2025-02-18T10:40:06.987563Z","iopub.status.idle":"2025-02-18T10:40:06.992098Z","shell.execute_reply.started":"2025-02-18T10:40:06.987538Z","shell.execute_reply":"2025-02-18T10:40:06.991199Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Seeting the seed","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)  \ntorch.cuda.manual_seed_all(42)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:06.993870Z","iopub.execute_input":"2025-02-18T10:40:06.994105Z","iopub.status.idle":"2025-02-18T10:40:07.008890Z","shell.execute_reply.started":"2025-02-18T10:40:06.994084Z","shell.execute_reply":"2025-02-18T10:40:07.008164Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"Importing the LLama 3.1 model. The BitsAndBytesConfig is used for quantization in Hugging Face and enables a more efficient memory footprint during model inference and training. We also load the model.","metadata":{}},{"cell_type":"code","source":"model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token \n\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,  \n    llm_int8_enable_fp32_cpu_offload=True)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    pad_token_id=tokenizer.eos_token_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\"  \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:07.010171Z","iopub.execute_input":"2025-02-18T10:40:07.010431Z","iopub.status.idle":"2025-02-18T10:40:25.402275Z","shell.execute_reply.started":"2025-02-18T10:40:07.010411Z","shell.execute_reply":"2025-02-18T10:40:25.401528Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3ab1e34eb24aa19e03b6c4683dc9b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3.1/transformers/8b-instruct/2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"LoRA (Low-Rank Adaptation) is a technique used to efficiently fine-tune large pre-trained models like transformers, especially those with billions of parameters. The goal of LoRA is to provide a more memory-efficient way of adapting large models to specific tasks without needing to fine-tune the entire model, which can be computationally expensive and resource-intensive.","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    inference_mode=False,\n    lora_alpha=32,\n    lora_dropout=0.05\n)\nlora_model = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:25.403148Z","iopub.execute_input":"2025-02-18T10:40:25.403449Z","iopub.status.idle":"2025-02-18T10:40:25.568209Z","shell.execute_reply.started":"2025-02-18T10:40:25.403420Z","shell.execute_reply":"2025-02-18T10:40:25.567307Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"Preparing the prompt and data","metadata":{}},{"cell_type":"code","source":"lora_model = get_peft_model(model, lora_config)\n\nprompt = \"\"\"Below is a sentence along with its language. Predict the sentiment as 1 for Positive or 0 for Negative.\n\n### Sentence:\n{}\n\n### Language:\n{}\n\n### Output (0 for Negative, 1 for Positive):\n\"\"\"\n\n\nEOS_TOKEN = tokenizer.eos_token  # Ensure EOS_TOKEN is added\n\n\ndf = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/train.csv')\n\nlabel_mapping = {\"Positive\": 1, \"Negative\": 0}\ndf[\"label\"] = df[\"label\"].map(label_mapping)\n\ndataset = Dataset.from_pandas(df)\n\ndef formatting_prompts_func(examples):\n    return {\"text\": [prompt.format(s, l) + EOS_TOKEN for s, l in zip(examples[\"sentence\"], examples[\"language\"])]}\n\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\ndataset = dataset.remove_columns([\"ID\", \"sentence\", \"language\"])  \n\ndataset = dataset.map(lambda x: {\"labels\": x[\"label\"]}, batched=True)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\",  \n        truncation=True, \n        max_length=256\n    )\n\ndataset = dataset.map(tokenize_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:49.621424Z","iopub.execute_input":"2025-02-18T10:40:49.621729Z","iopub.status.idle":"2025-02-18T10:40:50.456911Z","shell.execute_reply.started":"2025-02-18T10:40:49.621706Z","shell.execute_reply":"2025-02-18T10:40:50.456203Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b58a37dfa846b6a9f0f748df5a0130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bb1a34ac6d4a2bac2b6d6a56fb4045"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9051c9b87794ff1abfccf20430c2c26"}},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"Spliting and training the model","metadata":{}},{"cell_type":"code","source":"split = dataset.train_test_split(test_size=0.2)\n\ntraining_args = TrainingArguments( output_dir='lora_llama_1b_ct',\n                                  eval_strategy=\"steps\",\n                                  eval_steps=100,\n                                  num_train_epochs=1,\n                                  per_device_train_batch_size=3,\n                                  per_device_eval_batch_size=3,\n                                  bf16=False,\n                                  fp16=True,\n                                  tf32=False,\n                                  gradient_accumulation_steps=1,\n                                  adam_beta1=0.9,\n                                  adam_beta2=0.999,\n                                  learning_rate=2e-5,\n                                  weight_decay=0.01,\n                                  logging_dir='logs',\n                                  logging_strategy=\"steps\",\n                                  logging_steps = 100,\n                                  save_steps=100,\n                                  save_total_limit=20,\n                                  report_to='none',\n                                  label_names=[\"labels\"]\n                                )\n\ntrainer = Trainer(\n    model=lora_model,\n    args=training_args,\n    train_dataset=split[\"train\"],\n    eval_dataset=split[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:40:57.884406Z","iopub.execute_input":"2025-02-18T10:40:57.884880Z","iopub.status.idle":"2025-02-18T10:49:24.959524Z","shell.execute_reply.started":"2025-02-18T10:40:57.884811Z","shell.execute_reply":"2025-02-18T10:49:24.958597Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-33-f490769d13e9>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [267/267 08:24, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.716500</td>\n      <td>0.757425</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.666000</td>\n      <td>0.843714</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=267, training_loss=0.7430036023314972, metrics={'train_runtime': 506.684, 'train_samples_per_second': 1.579, 'train_steps_per_second': 0.527, 'total_flos': 8584903104921600.0, 'train_loss': 0.7430036023314972, 'epoch': 1.0})"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"Creating the classification Pipeline ","metadata":{}},{"cell_type":"code","source":"from transformers import TextClassificationPipeline\n\n\nlora_model.config.id2label = {0: \"Negative\", 1: \"Positive\"}\nlora_model.config.label2id = {\"Negative\": 0, \"Positive\": 1}\n\nclassifier = TextClassificationPipeline(\n    model=lora_model, \n    tokenizer=tokenizer,\n    framework='pt',\n    task=\"sentiment-analysis\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:49:30.647978Z","iopub.execute_input":"2025-02-18T10:49:30.648289Z","iopub.status.idle":"2025-02-18T10:49:30.655504Z","shell.execute_reply.started":"2025-02-18T10:49:30.648268Z","shell.execute_reply":"2025-02-18T10:49:30.654717Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nThe model 'PeftModel' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'HeliumForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification', 'Zamba2ForSequenceClassification'].\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"Reading the test dataset, preparing for prediction and saving the prediction in a csv.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/test.csv')\n\ntest_dataset = Dataset.from_pandas(test_df)\n\ntest_dataset = test_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\ntest_texts = test_dataset['text']\npredictions = classifier(test_texts, truncation=True, padding=True, batch_size=8)\n\ntest_results = pd.DataFrame({\"label\": [p['label'] for p in predictions]})\ntest_results['ID'] = test_dataset[\"ID\"]\n\n\nsubmission = test_results[['ID', 'label']]\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:49:35.338609Z","iopub.execute_input":"2025-02-18T10:49:35.338942Z","iopub.status.idle":"2025-02-18T10:50:08.282078Z","shell.execute_reply.started":"2025-02-18T10:49:35.338918Z","shell.execute_reply":"2025-02-18T10:50:08.281135Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afbd0e0c8185463193dc5da7c08fb51e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6e75bf869d4c10bdac0c2bd9c7e142"}},"metadata":{}}],"execution_count":35}]}